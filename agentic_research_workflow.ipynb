{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUwgxQ0FrLiu"
      },
      "source": [
        "# <font color='#3BB9FF'>Lightweight Agentic AI: Research Assistant Agent </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract\n",
        "In this project, I built a lightweight research agent powered by serverless LLM APIs. The goal was to automate the process of exploring a research topic — breaking it down into focused subtopics, expanding search queries, gathering information from the web, and generating a concise and refined summary.\n",
        "\n",
        "The agent is designed around a modular workflow, where each capability — topic breakdown, query expansion, search, summarization, and critique is implemented as a standalone tool. These tools are orchestrated together in a structured agentic pipeline that simulates reasoning, reflection, and iterative improvement.\n",
        "\n",
        "Through this project, I explored how large language models can be combined with real-world APIs to create intelligent, research-oriented assistants that operate autonomously within a defined workflow.\n",
        "This serves as an introduction to broader ideas around agentic AI, autonomous tool usage, and multi-step task execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project Overview:\n",
        "A modular LLM-based research agent that automates topic exploration, search, summarization, and refinement through structured reasoning and tool orchestration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_HRjUFcvCci"
      },
      "source": [
        "\n",
        "**Defining each tool as a function with the necessary parameters configurations and prompts.**\n",
        "1.   Topic Breakdown Tool :  Creating a tool that takes a broad research topic\n",
        "and breaks it down into smaller, more focused subtopics or subqueries. Using LLM to generate these subtopics based on the main topic.\n",
        "\n",
        "2.  Query Expansion Tool: Developing a tool to expand the subqueries generated by the Topic Breakdown Tool. The tool should generate related keywords, synonyms, and phrases to enhance the search results.\n",
        "\n",
        "3. Search Tool: Creating a wrapper around the Brave Search API. Please note that the free tier is 1000 queries/month. Consider creating a mock while developing, and switch to actually call the You API once the agent is more stable. Additionally, consider caching the search results.\n",
        "\n",
        "4.  Summarizer Tool : The agent generates the summary incorporating the search results.\n",
        "\n",
        "5. Critique Tool: Create a tool that critiques the summary, and offers suggestions of how to improve and potentially other relevant topics to search for.\n",
        "\n",
        "6. Summarizer Tool : Create a tool that takes some input and summarizes its content using an LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWGOetY6cO0H"
      },
      "source": [
        "### Tool Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting together\n",
            "  Downloading together-1.5.5-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.9.3 (from together)\n",
            "  Downloading aiohttp-3.11.18-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting click<9.0.0,>=8.1.7 (from together)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting eval-type-backport<0.3.0,>=0.1.3 (from together)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting filelock<4.0.0,>=3.13.1 (from together)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from together) (1.24.3)\n",
            "Collecting pillow<12.0.0,>=11.1.0 (from together)\n",
            "  Downloading pillow-11.2.1-cp311-cp311-macosx_10_10_x86_64.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from together) (11.0.0)\n",
            "Collecting pydantic<3.0.0,>=2.6.3 (from together)\n",
            "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from together) (2.31.0)\n",
            "Collecting rich<14.0.0,>=13.8.1 (from together)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting tabulate<0.10.0,>=0.9.0 (from together)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting tqdm<5.0.0,>=4.66.2 (from together)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typer<0.16,>=0.9 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from together) (0.9.4)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.9.3->together)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.2)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.9.3->together)\n",
            "  Downloading propcache-0.3.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.9.3->together)\n",
            "  Downloading yarl-1.20.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (72 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.6.3->together)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.6.3->together)\n",
            "  Downloading pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic<3.0.0,>=2.6.3->together)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.6.3->together)\n",
            "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/anvithahiriadka/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.0)\n",
            "Downloading together-1.5.5-py3-none-any.whl (87 kB)\n",
            "Downloading aiohttp-3.11.18-cp311-cp311-macosx_10_9_x86_64.whl (471 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading pillow-11.2.1-cp311-cp311-macosx_10_10_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
            "Downloading pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading propcache-0.3.1-cp311-cp311-macosx_10_9_x86_64.whl (46 kB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading yarl-1.20.0-cp311-cp311-macosx_10_9_x86_64.whl (96 kB)\n",
            "Installing collected packages: typing-extensions, tqdm, tabulate, propcache, pillow, filelock, eval-type-backport, click, annotated-types, aiohappyeyeballs, yarl, typing-inspection, rich, pydantic-core, pydantic, aiohttp, together\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.8.10\n",
            "    Uninstalling tabulate-0.8.10:\n",
            "      Successfully uninstalled tabulate-0.8.10\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 10.0.1\n",
            "    Uninstalling Pillow-10.0.1:\n",
            "      Successfully uninstalled Pillow-10.0.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.9.0\n",
            "    Uninstalling filelock-3.9.0:\n",
            "      Successfully uninstalled filelock-3.9.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.0.4\n",
            "    Uninstalling click-8.0.4:\n",
            "      Successfully uninstalled click-8.0.4\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.8.1\n",
            "    Uninstalling yarl-1.8.1:\n",
            "      Successfully uninstalled yarl-1.8.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.1\n",
            "    Uninstalling rich-13.7.1:\n",
            "      Successfully uninstalled rich-13.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.8\n",
            "    Uninstalling pydantic-1.10.8:\n",
            "      Successfully uninstalled pydantic-1.10.8\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.8.5\n",
            "    Uninstalling aiohttp-3.8.5:\n",
            "      Successfully uninstalled aiohttp-3.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n",
            "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.11.3 which is incompatible.\n",
            "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 annotated-types-0.7.0 click-8.1.8 eval-type-backport-0.2.2 filelock-3.18.0 pillow-11.2.1 propcache-0.3.1 pydantic-2.11.3 pydantic-core-2.33.1 rich-13.9.4 tabulate-0.9.0 together-1.5.5 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspection-0.4.0 yarl-1.20.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Installing package\n",
        "!pip install together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tqbV9b8yUHA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Connecting TogetherAI API through API key\n",
        "'''\n",
        "\n",
        "import os\n",
        "# Importing the Together library to interface with the Together API\n",
        "from together import Together\n",
        "# Importing userdata from Google Colab to securely store and access the API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize the Together client with the API key retrieved from Google Colab's userdata\n",
        "client = Together(api_key=userdata.get('TOGETHER_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc00icl7d1P8",
        "outputId": "b4a66942-b087-475f-97f2-08fce2b9af3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "#testing the working of model\n",
        "# Creating a chat completion request using the Together API\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"hello\"\n",
        "        }\n",
        "],\n",
        "    max_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.7,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1,\n",
        "    stop=[\"<|eot_id|>\"]\n",
        ")\n",
        "# Printing the generated response from the model\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sfyJquIlU7i"
      },
      "source": [
        "**Defining each tool as a function with the necessary parameters and descriptions.**\n",
        "\n",
        "#### - **Topic Breakdown Tool**\n",
        "This tool takes a broad research topic and uses a language model to break it down into five smaller, more focused subtopics. It helps the agent organize the research process by creating manageable and specific areas to explore, setting a clear foundation for deeper information retrieval.\n",
        "\n",
        "  **LLM used** : mistralai/Mixtral-8x7B-Instruct-v0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85UbI6RT2h6j"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Defining topic breakdown tool function\n",
        "1. Topic Breakdown Tool\n",
        "Creating a tool that takes a broad research topic and breaks it down into smaller, more focused subtopics or subqueries.\n",
        "Using LLM to generate these subtopics based on the main topic.\n",
        "'''\n",
        "\n",
        "def topic_breakdown_tool(topic):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Break down the following research topic into smaller 5 subtopics:{topic}\"}\n",
        "        ],\n",
        "    max_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.7,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1,\n",
        "    stop=[\"[/INST]\",\"</s>\"],\n",
        "    stream=False\n",
        "    )\n",
        "  return response.choices[0].message.content\n",
        "#print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZBwLVAw-Dz"
      },
      "source": [
        "#### - **Query Exapnsion tool**\n",
        "\n",
        "This tool takes the subtopics generated from the initial topic breakdown and expands them by suggesting related keywords, synonyms, and phrases. By enriching each subtopic with more search-friendly variations, it helps improve the depth and quality of information the agent can retrieve during the search phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFOphGVfBe9C"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "2. Query Expansion Tool\n",
        "Developing a tool to expand the subqueries generated by the Topic Breakdown Tool.\n",
        "The tool should generate related keywords, synonyms, and phrases to enhance the search results.\n",
        "'''\n",
        "\n",
        "def query_expansion_tool(subtopics):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate related 3 keywords, 3 synonyms and 3 phrases for all of the subtopics:{subtopics} and should follow the following format: {{'Subtopic':subtopic,'Keywords':keywords,'Synonyms':Synonyms,'Phrases':phrases}}\"}\n",
        "    ],\n",
        "    max_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.7,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1,\n",
        "    stop=[\"[/INST]\",\"</s>\"],\n",
        "    stream=False\n",
        "    )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9gEVAafhl0g",
        "outputId": "a9d8a810-a14e-471c-c74d-abdc760774cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_m1dCaxOh1"
      },
      "source": [
        "- **Search Tool**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA7qM_6r9dWZ"
      },
      "outputs": [],
      "source": [
        "#SEARCH TOOL USING YOU.COM\n",
        "import requests\n",
        "'''\n",
        "3. Search Tool: Create a wrapper around the You API.\n",
        "Please note that the free tier is 1000 queries/month.\n",
        "'''\n",
        "YOUR_API_KEY = userdata.get('YOU_API')\n",
        "search_cache = {}\n",
        "\n",
        "def search_tool(query):\n",
        "    if query in search_cache:\n",
        "        return search_cache[query]  # Return cached result\n",
        "    else:\n",
        "        headers = {\"X-API-Key\": YOUR_API_KEY}\n",
        "        params = {\"query\": query}\n",
        "        response = requests.get(f\"https://api.ydc-index.io/search?query={query}\",\n",
        "                                params=params,\n",
        "                                headers=headers,\n",
        "                                ).json()\n",
        "        search_cache[query] = response  # Cache the new result\n",
        "        return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV-wtrOSCWVl"
      },
      "source": [
        "#### - **Summary tool**\n",
        "\n",
        "This tool takes the raw information gathered from the search results and transforms it into a clear, concise summary. It helps organize scattered findings into a focused, easy-to-read paragraph that captures the essence of the research topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UXqwJjWCb-P"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "Summarizer Tool\n",
        "Creating a tool that generates the summary incorporating the search results.\n",
        "'''\n",
        "\n",
        "def summary_tool(search_results):\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Summarize the following search result:{search_results}\"}\n",
        "        ],\n",
        "      max_tokens=512,\n",
        "      temperature=0.7,\n",
        "      top_p=0.7,\n",
        "      top_k=50,\n",
        "      repetition_penalty=1,\n",
        "      stop=[\"[/INST]\",\"</s>\"],\n",
        "      stream=False)\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7n5DL2iOQgW"
      },
      "source": [
        "#### - **Critique Tool**\n",
        "This tool reviews the generated summary by evaluating its conciseness, accuracy, coherence, and completeness. It provides constructive feedback on how the summary can be improved and suggests additional areas or topics worth exploring, helping the agent refine and strengthen the final output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9dc5J-XOgjT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Critique Tool\n",
        "The agent critiques the summary, and improves the results.\n",
        "'''\n",
        "\n",
        "def critique_tool(summary):\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Please critique the following summary in detail. Consider the following aspects in your critique:\n",
        "        1. Conciseness: Is the summary concise and to the point without losing essential information?\n",
        "        2. Accuracy: Are the facts and information in the summary accurate and well-researched?\n",
        "        3. Coherence: Does the summary flow logically from one point to the next?\n",
        "        4. Completeness: Does the summary cover all the key points of the topic?\n",
        "        Additionally, suggest any improvements that could be made and mention any relevant topics or areas that could be further explored.\n",
        "        Summary: {summary}\"\"\"}\n",
        "        ],\n",
        "      max_tokens=512,\n",
        "      temperature=0.7,\n",
        "      top_p=0.7,\n",
        "      top_k=50,\n",
        "      repetition_penalty=1,\n",
        "      stop=[\"[/INST]\",\"</s>\"],\n",
        "      stream=False)\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tuBKfuJxxcL"
      },
      "source": [
        "#### - **Final Summarizer Tool**\n",
        "This tool refines the initial summary by incorporating feedback from the critique. It uses the suggested improvements to polish the summary, ensuring the final output is more accurate, complete, and well-structured.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIMLVzTpwdzL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Summarizer Tool\n",
        "Creating a tool that generates the final summary using the critic results.\n",
        "'''\n",
        "\n",
        "def final_summary_tool(summary, critique):\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Improve the following summary: {summary} based on this critique suggestions: {critique}\"}\n",
        "        ],\n",
        "      max_tokens=512,\n",
        "      temperature=0.7,\n",
        "      top_p=0.7,\n",
        "      top_k=50,\n",
        "      repetition_penalty=1,\n",
        "      stop=[\"[/INST]\",\"</s>\"],\n",
        "      stream=False)\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Qo47pb_L_Y"
      },
      "source": [
        "#### Agent Workflow\n",
        "\n",
        "This workflow guides the research agent through a structured series of steps to transform a broad research topic into a polished, refined summary. Starting with breaking down the topic into subtopics, the agent then expands these into richer queries, searches for relevant information, and generates an initial summary. It further critiques the summary for quality and incorporates the feedback to improve the final output. By following this process, the agent ensures that the research is thorough, organized, and thoughtfully refined before presenting the final results to the user.\n",
        "\n",
        "Workflow:\n",
        "1. The agent receives a research topic from the user.\n",
        "2. It uses the Topic Breakdown Tool to generate subtopics or subqueries.\n",
        "3. The Query Expansion Tool expands the subqueries with related keywords and phrases.\n",
        "4. The Search Tool uses the expanded queries and subqueries to gather relevant information from various sources.\n",
        "5. The agent generates the summary incorporating the search results. (optional)\n",
        "6. The agent critiques the summary, and improves the results. (optional)\n",
        "7. The agent presents the final summary to the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8N2jL437L7j"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Agent Workflow\n",
        "Using all the tools defined above to implement an agent workflow.\n",
        "Overall this entire system acts as the \"Research Agent\"\n",
        "'''\n",
        "#Agent Workflow\n",
        "def research_agent(topic):\n",
        "  # Tool 1: Topic Breakdown\n",
        "  subtopics = topic_breakdown_tool(topic)\n",
        "\n",
        "  # Tool 2: Query Expansion\n",
        "  expanded_queries = query_expansion_tool(subtopics)\n",
        "\n",
        "  # Tool 3: Search information\n",
        "  results = search_tool(expanded_queries)\n",
        "  search_results = results['hits']\n",
        "\n",
        "  #Tool 4: Summarize the search results\n",
        "  summary = summary_tool(search_results)\n",
        "\n",
        "  #Tool 5: Critique the summary\n",
        "  critique = critique_tool(summary)\n",
        "\n",
        "  #Tool 6: Final Summarization\n",
        "  final_summary_results = final_summary_tool(summary, critique)\n",
        "\n",
        "  return {\n",
        "      \"subtopics\": subtopics,\n",
        "      \"expanded_queries\": expanded_queries,\n",
        "      \"search_results\": search_results,\n",
        "      \"summarized_results\":summary,\n",
        "      \"critique_results\":critique,\n",
        "      \"final_summary\":final_summary_results}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2ngjRDr9ciW"
      },
      "outputs": [],
      "source": [
        "# Assigning research topic to a variable and passing the to the research agent function\n",
        "topic = \"Future Trends and Innovations in LLM Research\"\n",
        "result = research_agent(topic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuMu72Qc8uAf",
        "outputId": "17c85d95-8043-459f-eac1-82fa8b868dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mRearch Topic ::Future Trends and Innovations in LLM Research\u001b[0m\n",
            "\n",
            "\u001b[1mResults from the topic_breakdown_tool ::\u001b[0m\n",
            "  Sure, I'd be happy to help you break down the research topic of \"Future Trends and Innovations in LLM Research\" into smaller subtopics. Here are five potential subtopics:\n",
            "\n",
            "1. Advances in Natural Language Processing (NLP) Techniques: This subtopic could cover the latest developments in NLP techniques and how they might be applied to LLM research. This could include machine learning algorithms, deep learning architectures, and other computational approaches to language processing.\n",
            "2. Emerging Applications of LLM in Industry: This subtopic could explore how LLM is being used in various industries, such as healthcare, finance, and education. It could also consider potential future applications of LLM in emerging fields like virtual reality and augmented reality.\n",
            "3. Ethical and Social Implications of LLM: As LLM becomes more prevalent, it's important to consider the ethical and social implications of this technology. This subtopic could examine issues such as bias in language models, privacy concerns, and the impact of LLM on employment and society.\n",
            "4. Multimodal LLM: This subtopic could explore the integration of different modalities, such as text, speech, and images, in LLM research. It could also consider the potential benefits and challenges of developing multimodal language models.\n",
            "5. Theoretical Foundations of LLM: This subtopic could delve into the theoretical underpinnings of LLM research, including linguistic theory, cognitive science, and artificial intelligence. It could also consider how these theories inform the development of LLM models and applications.\n",
            "\n",
            "\n",
            "\u001b[1mResults from the query_expansion_tool ::\u001b[0m\n",
            "  Sure, here are the keywords, synonyms, and phrases for each of the five subtopics:\n",
            "\n",
            "1. Advances in Natural Language Processing (NLP) Techniques:\n",
            "\n",
            "{'Subtopic':subtopic,'Keywords':['NLP techniques','machine learning','deep learning'],'Synonyms':['natural language processing methods','computational linguistics'],'Phrases':['advancements in NLP','innovations in machine learning','deep learning architectures']}\n",
            "\n",
            "2. Emerging Applications of LLM in Industry:\n",
            "\n",
            "{'Subtopic':subtopic,'Keywords':['industry applications','healthcare','finance'],'Synonyms':['practical uses','real-world applications'],'Phrases':['LLM in healthcare','financial applications of LLM','emerging industries for LLM']}\n",
            "\n",
            "3. Ethical and Social Implications of LLM:\n",
            "\n",
            "{'Subtopic':subtopic,'Keywords':['ethical implications','social implications','bias'],'Synonyms':['moral implications','societal impact'],'Phrases':['LLM and bias','ethical considerations in LLM','social consequences of LLM']}\n",
            "\n",
            "4. Multimodal LLM:\n",
            "\n",
            "{'Subtopic':subtopic,'Keywords':['multimodal LLM','text','speech'],'Synonyms':['multi-modal LLM','text-to-speech'],'Phrases':['integration of different modalities in LLM','multimodal language models','text and speech processing']}\n",
            "\n",
            "5. Theoretical Foundations of LLM:\n",
            "\n",
            "{'Subtopic':subtopic,'Keywords':['theoretical foundations','linguistic theory','cognitive science'],'Synonyms':['conceptual underpinnings','theoretical frameworks'],'Phrases':['theory of LLM','linguistic theories in LLM','cognitive science and LLM']}\n",
            "\n",
            "\n",
            "\u001b[1mResults from the search_tool ::\u001b[0m\n",
            " [{'description': 'Why do we need to even think about linguistics in NLP? Most NLP is dominated by a more tech and algorithm focus. At times we tend to lose track of the beauty and complexity of the task that computers…', 'snippets': ['Why do we need to even think about linguistics in NLP? Most NLP is dominated by a more tech and algorithm focus. At times we tend to lose…', 'The primary objective of NLP is to take in text or speech and develop machine learning algorithms for large scale and efficient processing of the inputs to give computers the ability to interpret, manipulate, and comprehend human language. NLP techniques: NLP is Multi-disciplinary — involving CS, linguistics, machine learning, Statistics and Maths. NLP techniques have shown significant evolution going from hard coded linguistic inspired rules in the 1950s, statistical machine learning techniques in the 1990s and finally to deep learning techniques from 2010 onwards.', 'The deep learning phase began with shallow neural networks used in Word2Vec models and later evolved to seq to seq models- such as RNNS and of course the hugely successful Transformer architecture and self-attention mechanisms that are SOTA today. ... The main output, particularly of NLP, is enabling computers to have an understanding of language which is then used to automate many tasks such as machine translation, NLU, NLG, text summarization, classification, etc.', 'At times it seems the two are doing the same thing. Especially, as in recent years computational linguistics is also leveraging deep learning methods. Many areas are common between CL and NLP. For example, there has been extensive research in computational linguistics on machine translation and NLU.'], 'title': 'NLP and Linguistics 1 — Just How Much Linguistics Do We Need to Know? | by Shailey Dash | Medium', 'url': 'https://medium.com/@shaileydash/nlp-and-linguistics-1-just-how-much-linguistics-do-we-need-to-know-7931a4a16de6'}, {'description': 'Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics. The goal is for computers to process or “understand” natural language…', 'snippets': ['Deep Learning provides a very flexible, universal, and learnable framework for representing the world for visual and linguistic information. Initially, it resulted in breakthroughs in fields such as speech recognition and computer vision. Recently, deep learning approaches have obtained very high performance across many different NLP tasks.', 'In fact, most of the NLP problems can be considered as a question answering problem. The paradigm is simple: we issue a query, and the machine responds. By reading through a document, or a set of instructions, an intelligent system should be able to answer a wide variety of questions. So naturally, we’d like to design a model that can be used for general QA. ... A powerful deep learning architecture, known as dynamic memory network(DMN), has been developed and optimized specifically for QA problems.', 'I showed you a basic rundown of the major natural language processing techniques that can help a computer extract, analyze, and understand useful information from a single text or sequence of texts. From machine translation that connects humans across cultures to conversational chatbots that help with customer service; from sentiment analysis that deeply understands a human’s mood to attention mechanisms that can mimic our visual attention, the field of NLP is too expansive to cover completely, so I’d encourage you to explore it further, whether through online courses, blog tutorials, or research papers.', 'Human languages are ambiguous (unlike programming and other formal languages); thus there is a high level of complexity in representing, learning, and using linguistic/situational/contextual / word / visual knowledge towards the human language. There’s a fast-growing collection of useful applications derived from this field of study. They range from simple to complex. Below are a few of them: Spell Checking, Keyword Search, Finding Synonyms.'], 'title': 'Natural Language Processing: Advance Techniques ~ In-Depth Analysis. | by Nadeem | Analytics Vidhya | Medium', 'url': 'https://medium.com/analytics-vidhya/natural-language-processing-advance-techniques-in-depth-analysis-b67bca5db432'}, {'description': 'Natural language processing (NLP) is a subfield of artificial intelligence (AI) that uses machine learning to help computers communicate with human language.', 'snippets': ['NLP enables computers and digital devices to recognize, understand and generate text and speech by combining computational linguistics—the rule-based modeling of human language—together with statistical modeling, machine learning (ML) and deep learning.', 'NLP combines the power of computational linguistics together with machine learning algorithms and deep learning. Computational linguistics is a discipline of linguistics that uses data science to analyze language and speech. It includes two main types of analysis: syntactical analysis and semantical analysis.', 'This relies on machine learning, enabling a sophisticated breakdown of linguistics such as part-of-speech tagging. Statistical NLP introduced the essential technique of mapping language elements—such as words and grammatical rules—to a vector representation so that language can be modeled by using mathematical (statistical) methods, including regression or Markov models. This informed early NLP developments such as spellcheckers and T9 texting (Text on 9 keys, to be used on Touch-Tone telephones). Deep learning NLP: Recently, deep learning models have become the dominant mode of NLP, by using huge volumes of raw, unstructured data—both text and voice—to become ever more accurate.', 'NLG: the differences between three natural language processing concepts” for a deeper look into how these concepts relate. ... The all-new enterprise studio that brings together traditional machine learning along with new generative AI capabilities powered by foundation models. ... Organizations can use NLP to process communications that include email, SMS, audio, video, newsfeeds and social media. NLP is the driving force behind AI in many modern real-world applications. Here are a few examples:'], 'title': 'What Is NLP (Natural Language Processing)? | IBM', 'url': 'https://www.ibm.com/topics/natural-language-processing'}, {'description': 'Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. It is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge ...', 'snippets': [\"This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models.\", 'It is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.', 'Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.', 'Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL).', \"The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.\"], 'title': 'Natural language processing - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Natural_language_processing'}, {'description': \"Medallia's text analytics software tool provides actionable insights via customer and employee experience sentiment data analysis from reviews & comments.\", 'snippets': ['Here’s how Medallia has innovated and iterated to build the most accurate, actionable, and scalable text analytics.', 'Medallia’s omnichannel Text Analytics with Natural Language Understanding and AI – powered by Athena – enables you to quickly identify emerging trends and key insights at scale for each user role in your organization.'], 'title': 'Natural Language Processing (NLP): 7 Key Techniques', 'url': 'https://monkeylearn.com/blog/natural-language-processing-techniques'}, {'description': 'How to select latest innovative natural language processing thesis topics? Sample Model approch technique used in NLP Projects.', 'snippets': ['As this is an important section, you are advised to pay your attention here. Are you really interested in getting into the next section? Come let us also learn them. ... These are some of the latest thesis topics in NLP. As the matter of fact, we have delivered around 200 to 300 thesis with fruitful outcomes. Actually, they are very innovative and unique by means of their features.', 'Generally, natural language processing is the sub-branch of Artificial Intelligence (AI). Natural language processing is otherwise known as NLP. It is compatible in dealing with multi-linguistic aspects and they convert the text into binary formats in which computers can understand it.', 'Generally, NLP processes are getting performed in a structural manner. That means they are overlays in several steps in crafting · natural language processing thesis topics. Yes dears, we are going to envelop the next section with the steps that are concreted with the natural language processing. ... Here POS stands for the Parts of Speech.', 'These are some of the steps involved in natural language processing. NLP performs according to the inputs given. Here you might need examples in these areas. For your better understanding, we are going to illustrate to you about the same with clear bulletin points.'], 'title': 'Latest 12+ Interesting Natural Language Processing Thesis Topics', 'url': 'https://phdservices.org/natural-language-processing-thesis-topics/'}, {'description': 'At a high level, NLU and NLG are just components of NLP. In this post, we’ll define each term individually and summarize their differences.', 'snippets': ['While computational linguistics has more of a focus on aspects of language, natural language processing emphasizes its use of machine learning and deep learning techniques to complete tasks, like language translation or question answering. Natural language processing works by taking unstructured data and converting it into a structured data format.', 'At a high level, NLU and NLG are just components of NLP. Given how they intersect, they are commonly confused within conversation, but in this post, we’ll define each term individually and summarize their differences to clarify any ambiguities. ... Natural language processing, which evolved from computational linguistics, uses methods from various disciplines, such as computer science, artificial intelligence, linguistics, and data science, to enable computers to understand human language in both written and verbal forms.', 'While natural language understanding focuses on computer reading comprehension, natural language generation enables computers to write. NLG is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from in-put documents while maintaining the integrity of the information. Extractive summarization is the AI innovation powering Key Point Analysis used in That’s Debatable.', 'Natural language processing (NLP) seeks to convert unstructured language data into a structured data format to enable machines to understand speech and text and formulate relevant, contextual responses. Its subtopics include natural language processing and natural language generation.'], 'title': \"NLP vs. NLU vs. NLG: What's the Difference? | IBM\", 'url': 'https://www.ibm.com/blog/nlp-vs-nlu-vs-nlg-the-differences-between-three-natural-language-processing-concepts/'}, {'description': 'To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design.This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the ...', 'snippets': ['81. North American Chapter of the Association for Computational Linguistics Active Learning for NLP. 2020. http://active-learning.net/alnlp2010 (accessed 7 Jun 2011). [Google Scholar] 82. Fodor P, Lally A, Ferrucci D. The Prolog Interface to the Unstructured Information Management Architecture.', 'Keywords: Natural language processing, Introduction, clinical NLP, knowledge bases, machine learning, predictive modeling, statistical learning, privacy technology · This tutorial provides an overview of natural language processing (NLP) and lays a foundation for the JAMIA reader to better appreciate the articles in this issue. NLP began in the 1950s as the intersection of artificial intelligence and linguistics.', 'This problem, however, applies to NLP in general: it would occur even if the individual tasks were all combined into a single body of code. One way to address it (adopted in some commercial systems) is to use alternative algorithms (in multiple or branching pipelines) and contrast the final results obtained. This allows tuning the output to trade-offs (high precision versus high recall, etc). Recent advances in artificial intelligence (eg, computer chess) have shown that effective approaches utilize the strengths of electronic circuitry—high speed and large memory/disk capacity, problem-specific data-compression techniques and evaluation functions, highly efficient search—rather than trying to mimic human neural function.', 'By contrast, NLP toolkits and UIMA are still oriented toward the advanced programmer, and commercial offerings are expensive. General purpose NLP is possibly overdue for commoditization: if this happens, best-of-breed solutions are more likely to rise to the top. Again, analytics vendors are likely to lead the way, following the steps of biomedical informatics researchers to devise innovative solutions to the challenge of processing complex biomedical language in the diverse settings where it is employed.'], 'title': 'Natural language processing: an introduction - PMC', 'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168328/'}, {'description': 'Natural Language Processing is the discipline of building machines that can manipulate language in the way that it is written, spoken, and organized', 'snippets': ['Mixture of Experts (MoE): While most deep learning models use the same set of parameters to process every input, MoE models aim to provide different parameters for different inputs based on efficient routing algorithms to achieve higher performance. Switch Transformer is an example of the MoE approach that aims to reduce communication and computational costs. Many languages and libraries support NLP. Here are a few of the most useful.', 'View Course Course Deep Learning Specialization An intermediate set of five courses that help learners get hands-on experience building and deploying neural networks, the technology at the heart of today’s most advanced NLP and other sorts of AI models.', 'High cost leaves out non-corporate researchers: The computational requirements needed to train or deploy large language models are too expensive for many small companies. Some experts worry that this could block many capable engineers from contributing to innovation in AI.  · Black box: When a deep learning model renders an output, it’s difficult or impossible to know why it generated that particular result.', 'Most libraries and frameworks for deep learning are written for Python. Here are a few that practitioners may find helpful: Natural Language Toolkit (NLTK) is one of the first NLP libraries written in Python. It provides easy-to-use interfaces to corpora and lexical resources such as WordNet. It also provides a suite of text-processing libraries for classification, tagging, stemming, parsing, and semantic reasoning. spaCy is one of the most versatile open source NLP libraries.'], 'title': 'Natural Language Processing (NLP) [A Complete Guide]', 'url': 'https://www.deeplearning.ai/resources/natural-language-processing/'}, {'description': 'A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.', 'snippets': ['Natural language processing (NLP) is a field of computer science and a subfield of artificial intelligence that aims to make computers understand human language. NLP uses computational linguistics, which is the study of how language works, and various models based on statistics, machine learning, and deep learning.', 'From basic tasks like tokenization and part-of-speech tagging to advanced applications like sentiment analysis and machine translation, the impact of NLP is evident across various domains. As the technology continues to evolve, driven by advancements in machine learning and artificial intelligence, the potential for NLP to enhance human-computer interaction and solve complex language-related challenges remains immense.', 'Sentiment Analysis: A technique to understand the emotions or opinions expressed in a piece of text, by using various techniques like Lexicon-Based, Machine Learning-Based, and Deep Learning-based methods · Machine Translation: NLP is used for language translation from one language to another through a computer.', 'NLP models are computational systems that can process natural language data, such as text or speech, and perform various tasks, such as translation, summarization, sentiment analysis, etc. NLP models are usually based on machine learning or deep learning techniques that learn from large amounts of language data.'], 'title': 'Natural Language Processing (NLP) - Overview - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/natural-language-processing-overview/'}]\n",
            "\n",
            "\n",
            "\u001b[1mResults from the summary_tool ::\u001b[0m\n",
            "  The search results discuss the field of Natural Language Processing (NLP), which is at the intersection of computer science, artificial intelligence, and linguistics. NLP enables computers to process and understand human language by combining computational linguistics, statistical modeling, machine learning, and deep learning. The goal is to convert unstructured language data into a structured data format to enable machines to understand speech and text and formulate relevant, contextual responses.\n",
            "\n",
            "The evolution of NLP techniques is also discussed, with early developments in the 1950s and 1990s using rule-based and statistical approaches, and more recent advancements in deep learning models. These models use huge volumes of raw, unstructured data to become increasingly accurate in tasks such as machine translation, question answering, and sentiment analysis.\n",
            "\n",
            "Additionally, the search results mention some applications of NLP, such as spell checking, keyword search, and finding synonyms, as well as more complex applications in fields such as biomedical informatics.\n",
            "\n",
            "Finally, the search results also touch on some challenges in NLP, such as the high level of complexity in representing and using linguistic knowledge due to the ambiguity of human languages, and the high computational requirements needed to train or deploy large language models.\n",
            "\n",
            "\n",
            "\u001b[1mResults from the critique_tool ::\u001b[0m\n",
            "  The summary is well-written and effectively covers the main points of Natural Language Processing (NLP). Here are some comments and suggestions for improvement:\n",
            "\n",
            "1. Conciseness: The summary is concise and to the point. However, it could be further improved by shortening some sentences or removing redundant words. For example, the phrase \"and formulate relevant, contextual responses\" in the first paragraph could be omitted without losing any essential information.\n",
            "2. Accuracy: The summary is generally accurate, but there are a few minor issues. For instance, the statement \"The goal is to convert unstructured language data into a structured data format\" could be more precise. A more accurate goal of NLP is to enable machines to understand, interpret, and generate human language in a valuable way.\n",
            "3. Coherence: The summary flows logically from one point to the next. However, it might be helpful to provide more context or background information about NLP to make the summary more accessible to readers who are not familiar with the topic.\n",
            "4. Completeness: The summary covers most of the key points of NLP, including its definition, evolution, applications, and challenges. However, it could benefit from mentioning some of the latest trends and developments in NLP, such as the use of transformer models, transfer learning, and few-shot learning.\n",
            "\n",
            "Overall, the summary is informative and well-written. To further explore the topic, readers could delve into the specific techniques and algorithms used in NLP, as well as the ethical and social implications of using NLP systems.\n",
            "\n",
            "\n",
            "\u001b[1mResults from the final_summary_tool ::\u001b[0m\n",
            "  Natural Language Processing (NLP) is a field that combines computer science, artificial intelligence, and linguistics to enable computers to process and understand human language. NLP techniques convert unstructured language data into a format that machines can understand, allowing them to interpret speech and text and generate relevant responses.\n",
            "\n",
            "Early NLP techniques in the 1950s and 1990s used rule-based and statistical approaches, while more recent advancements leverage deep learning models that use vast amounts of raw, unstructured data for tasks such as machine translation, question answering, and sentiment analysis.\n",
            "\n",
            "NLP has various applications, including spell checking, keyword search, and finding synonyms, as well as more complex uses in fields like biomedical informatics. However, NLP faces challenges such as the complexity of linguistic knowledge representation and high computational requirements for training and deploying large language models.\n",
            "\n",
            "To enhance this summary, consider adding the latest trends and developments in NLP, such as transformer models, transfer learning, and few-shot learning. Additionally, discussing specific techniques and algorithms used in NLP and its ethical and social implications can provide further insight into the topic.\n"
          ]
        }
      ],
      "source": [
        "#Printing all the results generated from the above tools ---> working flow results of Research Agent\n",
        "\n",
        "print (f\"\\033[1mRearch Topic ::{topic}\\033[0m\\n\")\n",
        "print(f\"\\033[1mResults from the topic_breakdown_tool ::\\033[0m\\n {result['subtopics']}\")\n",
        "print(\"\\n\")\n",
        "print (f\"\\033[1mResults from the query_expansion_tool ::\\033[0m\\n {result['expanded_queries']}\")\n",
        "print(\"\\n\")\n",
        "print (f\"\\033[1mResults from the search_tool ::\\033[0m\\n {result['search_results']}\")\n",
        "print(\"\\n\")\n",
        "print (f\"\\033[1mResults from the summary_tool ::\\033[0m\\n {result['summarized_results']}\")\n",
        "print(\"\\n\")\n",
        "print (f\"\\033[1mResults from the critique_tool ::\\033[0m\\n {result['critique_results']}\")\n",
        "print(\"\\n\")\n",
        "print (f\"\\033[1mResults from the final_summary_tool ::\\033[0m\\n {result['final_summary']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im9GFOWnMyfn"
      },
      "source": [
        "## <font color='#3CB371'> **Conclusion:** </font>\n",
        "\n",
        "In this project, I developed six modular tools — the Topic Breakdown Tool, Query Expansion Tool, Search Tool, Summary Tool, Critique Tool, and Final Summarizer Tool — each designed to handle a key step in the research process.\n",
        "\n",
        "I then brought these tools together by creating a function that acts as a Research Agent, capable of taking a user-provided research topic and autonomously performing the entire research workflow.\n",
        "\n",
        "Throughout the process, intermediate outputs from each tool were displayed to show how the research evolved at every stage, culminating in a polished final summary that captures the essence of the original topic.\n",
        "\n",
        "This project also touches upon early principles of **agentic AI**, where an agent autonomously uses multiple tools, reasons through steps, reflects on its output, and refines its results — moving beyond simple single-turn LLM responses toward a more structured, goal-driven workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0MupCrnYWCA"
      },
      "source": [
        "**********************\n",
        "END OF PROJECT\n",
        "**********************"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
